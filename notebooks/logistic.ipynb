{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using logistic regression\n",
    "For this exercise we'll be using [scikit-learn](https://scikit-learn.org/stable/). This is great library/module for predictive data analysis in Python. It's got a huge library of algorithms and models, from standard linear models like regression, to Support Vector Machines, Decision Trees, and Neural Networks. We'll start with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of image data\n",
    "When working with images in machine learning, it's common to apply some kind of normalization to the pixel values.\n",
    "One of the common ways to normalize image data is to scale the values to the [0, 1] interval. This is what we will do here,\n",
    "using min-max scaling.\n",
    "Min-max scaling preserves the shape of the distribution, so that the difference between each image in\n",
    "a set of images is also preserved. In doing so, we are assuming that this property in the data is important,\n",
    "but keep in mind that it is something we can change, should we want to.\n",
    "Min-max scaling is calculated as\n",
    "$$\\text{scaled image} = \\frac{\\text{image} - \\mu_{image}}{I_{max} - I_{min}},$$\n",
    "where $I_{max}$ and $I_{min}$ refer to the maximum and minimum pixel intensity,\n",
    "and $\\mu_{image}$ is the mean pixel intensity for the set of images.\n",
    "\n",
    "We'll implement this as a function `normalize_image_data()`.\n",
    "However, we're not going to use it quite yet. We won't apply normalization to the data before we've split it into\n",
    "a training set and a validation set. If we normalize the entire dataset, then split it, we've techinically\n",
    "included properties of the validation set in the training set. This can give you a false picture of how well\n",
    "your model is performing, and should be avoided.\n",
    "\n",
    "Note that sklearn has a function for this too, an entire library for data preprocessing, actually.\n",
    "Check it out [here](https://scikit-learn.org/stable/modules/preprocessing.html).\\\n",
    "We're also going to save this normalization function to a separate python file called `helper_functions.py`.\n",
    "That way, we don't need to define it in every single notebook we make. We can just import it from that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the normalization function\n",
    "def normalize_image_data(images):\n",
    "    \"\"\" Takes an imported set of images and normalizes values to between\n",
    "    0 and 1 using min-max scaling across the whole image set.\n",
    "    \"\"\"\n",
    "    img_term = np.amax(images) - np.amin(images)\n",
    "    img_mean = np.mean(images)\n",
    "    images = (images - img_mean) / img_term\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels.\n",
    "DATA_PATH = \"../data/\"\n",
    "\n",
    "images = np.load(DATA_PATH+\"images_training.npy\")\n",
    "labels = np.load(DATA_PATH+\"labels_training.npy\")\n",
    "\n",
    "# Split the training indices into training and validation. \n",
    "# Validate with 25% of the data (default). Can be adjusted.\n",
    "x_idx = np.arange(images.shape[0])\n",
    "train_idx, val_idx, not_used1, not_used2 = train_test_split(x_idx, x_idx, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit to the training data\n",
    "The documentation for the LogistigRegression class is available [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression).\n",
    "\n",
    "We'll start off with the default settings for the algorithm, and once we've got everything working we'll take a look\n",
    "at tuning the *hyperparameters*.\n",
    "\n",
    "Recall the shape of the images we stored in the data_import_exploration notebook. It's (n_images, 16, 16), but the regression\n",
    "class expect a vector, so to input them we need to reshape the array again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.reshape(images.shape[0], 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init logreg class and fit to the training data.\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# We pass normalized image data to the function.\n",
    "logreg.fit(normalize_image_data(images[train_idx]), labels[train_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model\n",
    "To go further into the backround for the chosen metrics, we need to establish four quantities that appear in\n",
    "most, if not all discussion of the topic. These are:\n",
    "* True positive (TP) - Double event classified as double event\n",
    "* True negative (TN) - Single event classified as single event\n",
    "* False positive (FP) - Single event classified as double event\n",
    "* False negative (FN) - Double event classified as single event\n",
    "\n",
    "Which type of event is \"positive\" and \"negative\" is an arbitrary choice. We have chosen to label our events\n",
    "such that 0 = single and 1 = double. Those of Scikit-Learn's metrics which use the class labels treat\n",
    "the class labeled as $1$ as the positive class, which we'll see later.\n",
    "\n",
    "From these terms we can define some properties of a classifier:\\\n",
    "**Sensitivity** (or **True positive rate** (TPR), or **recall**) measures the fraction of positive samples in the data\n",
    "that are correctly classified as positive.\n",
    "\n",
    "$$\\text{sensitivity} = \\frac{\\text{number of true positives}}{\\text{number of true positives} + \\text{number of false negatives}} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**Specificity** (or **True negative rate** (TNR)) measures the fraction of negative samples in the data that are correctly classified as negative.\n",
    "\n",
    "$$\\text{specificity} = \\frac{\\text{number of true negatives}}{\\text{number of true negatives} + \\text{number of false positives}} = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "**Precision** (or **Positive preditive value** (PPV)) measures the fraction of samples classified as positive\n",
    "that are correctly classified.\n",
    "\n",
    "$$\\text{precision} = \\frac{\\text{number of true positives}}{\\text{number of true positives} + \\text{number of false positives}} = \\frac{TP}{TP + FP}$$\n",
    "## Accuracy\n",
    "The accuracy is a well known measure of performance, but not always a good one. It's simply the fraction of\n",
    "all samples that were correctly classified. Or, using the terms above\n",
    "$$\\text{accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "This is the default score for the LogisticRegression class in scikit-learn, but is also available as a standalone\n",
    "function in Scikit-Learn's metrics as [accuracy_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Make a set of predictions to evaluate\n",
    "pred = logreg.predict(normalize_image_data(images[val_idx]))\n",
    "\n",
    "# Convert sigmoid values from classification to integers so it works with the metric functions.\n",
    "result = pred > 0.5\n",
    "acc = accuracy_score(labels[val_idx], result)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it not always a good performance metric?\n",
    "\n",
    "Suppose you have a classifier which predicts every sample to be a double event. If you apply this classifier to\n",
    "a set of events that is not balanced you will get a false picture of its performance.\n",
    "\n",
    "Example:\\\n",
    "For a set of 15 single events and 85 double events, the accuracy of this particular classifier would be 85%!\n",
    "But it's clearly a terrible classifier.\n",
    "\n",
    "So what can we do instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "The confusion matrix can be a useful metric to gain a little bit\n",
    "more insight into specifically what the model gets wrong (or right).\n",
    "\n",
    "The confusion matrix is an n by n matrix containing correct classifications (true)\n",
    "on the diagonal, and false positives and negatives in the off-diagonal elements.\n",
    "An example of such a matrix could be the following table:\n",
    "\n",
    "|                  | True Single | True Double |\n",
    "|:-----------------|:-----------:|:-----------:|\n",
    "| **Classified Single** | 5 (TN) | 1 (FN)      |\n",
    "| **Classified Double** | 3 (FP) | 6 (TP)      |\n",
    "\n",
    "In the table above, the diagonal elements $i = j$ are the correct classifications, \n",
    "while the other elements correspond to cases where the model predicted class \n",
    "$i$, but should've predicted class $j$. The confusion matrix thus gives information about \n",
    "false positives and false negatives, in addition to classification accuracy. \n",
    "This is very useful in cases where for example false positives can be readily ignored or \n",
    "filtered later, but false negatives may have severe consequences. An example of this\n",
    "could be detection of cancer, in which a false positive can be ruled out from further testing, \n",
    "while a false negative may lead to a patient being sent home when actually needing help.\n",
    "\n",
    "We can compute the confusion matrix using Scikit-Learn's [confusion matrix()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(labels[val_idx], result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 score\n",
    "The F1 score is a also a measure of accuracy of the model, but it accounts for more than regular accuracy.\n",
    "It is defined as\n",
    "\n",
    "$$F_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}},$$\n",
    "\n",
    "which is the harmonic mean of precision and recall. Why is this better than accuracy? By including both precision and\n",
    "recall, you are combining two metrics that both tell you something about how good the model is at classifying your \"positive\" class. For our case of single and double events, the default behaviour of scikit-learn's f1_score function tells us about\n",
    "how good the model is at classifying double events. We can also specify single events as the \"positive\" class to get some\n",
    "insight into that aswell. Scikit-Learn provides this metric through [f1_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "score_double = f1_score(labels[val_idx], result)\n",
    "print(\"F1-score, double as positive class: \", score_double)\n",
    "\n",
    "score_single = f1_score(labels[val_idx], result, pos_label=0)\n",
    "print(\"F1-score, single as positive class: \", score_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the F1 score doesn't take into account the true negatives that can be seen in the confusion matrix, it has met\n",
    "some critisism, especially for use in binary classification.\n",
    "A common metric that is recommended to use instead, is Matthews Correlation Coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matthews correlation coefficient (MCC)\n",
    "MCC is tailored to measure the quality of a binary classifier, and is popular in bioinformatics. It accounts for both\n",
    "true and false positives and negatives, leading to a good overall metric if you have to boil the performance down\n",
    "to a single number. Additionally, the MCC is invariant to which class is defined as the positive one.\n",
    "It ranges between -1 and +1, where +1 indicates a perfect classifier, 0 is equivalent to random guessing, and -1\n",
    "is a classifier which classifies every sample wrong. -1 is essentially a perfect classifier that has been \"flipped\"\n",
    "(as long as it's a binary classifier).\n",
    "\n",
    "MCC is defined as\n",
    "$$\\text{MCC} = \\frac{ TP \\cdot TN - FP \\cdot FN } {\\sqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } },$$\n",
    "\n",
    "where the quantities are the same as those defined for the confusion matrix above.\n",
    "\n",
    "Scikit-Learn's metrics implement this as [matthews_corrcoef()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(labels[val_idx], result)\n",
    "print(\"MCC:\", mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC-curve and Area Under Curve\n",
    "The Receiver Operating Characteristic (ROC) is a widely used measure of a\n",
    "classifiers performance . The performance is measured as the effect\n",
    "of the true positive rate (TPR) and the false positive rate (FPR) as a function\n",
    "of thresholding the positive class. To evaluate the ROC curve for a model,\n",
    "traditionally the Area Under the Curve (AUC) is used, which ranges from 0\n",
    "(an ideal \"opposite\" classifier) to 1.0 (an ideal classifier) with 0.5\n",
    "indicating a random choice classifier.\n",
    "\n",
    "Scikit-Learn includes these metrics as [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html?highlight=roc_curve) and [roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html). They also have an article on [ROC](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py) with plot examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(labels[val_idx], pred)\n",
    "roc_auc = roc_auc_score(labels[val_idx], pred)\n",
    "# Plot the curve. Just like the history plot, we'll save this in helper_functions.py, too\n",
    "# as plot_roc_auc.\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=\"ROC curve (area = {:0.2f})\".format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label=\"Random classifier\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

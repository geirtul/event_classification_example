{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying simulated events using a Convolutional Neural Network\n",
    "We've seen how we can use logistig regression or feed forward neural networks (FFNN) to classify our data,\n",
    "and now you've also been introduced to what a convolutional neural network is.\n",
    "In this notebook, we're going to use a convolutional neural network (CNN) to classify our data,\n",
    "and even combine CNN and FFNN to expand our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from helper_functions import normalize_image_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels.\n",
    "DATA_PATH = \"../data/\"\n",
    "\n",
    "images = np.load(DATA_PATH+\"images_training.npy\")\n",
    "labels = np.load(DATA_PATH+\"labels_training.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The motivation for uing a CNN to classify this data comes from their huge success in image classification\n",
    "tasks in recent years. Since we can very easily (and even preferably) represent our data as images, it\n",
    "is a natural choice to try a model of this type.\n",
    "This notebook will be a bit more hands-on than the previous ones.\n",
    "\n",
    "You may recall from the lectures that one of the properties of CNNs is that they use 'filters'/'kernels' to\n",
    "extract 'features' from images. But how can we know what type of filters to apply? The answer is - we don't need to! Similar to how a FFNN adjusts its weights in order to increase performance, a CNN adjusts its filters and in that way 'learns' the best filters for a given task.\n",
    "\n",
    "Some very useful documentation to keep at hand while working through this notebook:\n",
    "* [tensorflow.keras.layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n",
    "* [Scikit-Learn model evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "Recall that when we saved our training samples in the import and exploration notebook,\n",
    "we saved the images as an array with the following dimensions (samples, x_pixels, y_pixels).\n",
    "This isn't necessarily a format the convolutional layer will accept, because by default image data\n",
    "has one more dimension - \"channels\". A single image would have the dimensions (x_pixels, y_pixels, channels).\n",
    "For a regular RGB picture this is (x_pixels, y_pixels, 3), since you have one channel for each color.\n",
    "Our data only has one channel, which in a world of colours would be called \"grayscale\".\n",
    "\n",
    "The convolutional layer we will use in our model is called Conv2D ([Doc](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)). Take a look at the documentation to determine if you need to modify the image array in order to input it to the layer.\n",
    "\n",
    "Recall that you can use [np.reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) to modify your array should you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the image array here, if you need to\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that can be useful to check is if we actually saved a balanced dataset. The data is shuffled\n",
    "when scikit-learn's train_test_split function splits it, so it's not guaranteed. Remember that if you\n",
    "don't have a balanced dataset, you need to account for this when concidering which metrics to use\n",
    "in evaluating your model.\n",
    "\n",
    "Numpy's [np.unique](https://numpy.org/doc/1.18/reference/generated/numpy.unique.html?) works great for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many there are of each class and print the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we split the training data into a training set and a validation set, using indices.\n",
    "Working with only 10000 you can also split the image array itself.\n",
    "As in the previous notebooks, [Scikit-Learn's train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) will do the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Having a common API for machine learning operations between multiple frameworks is great.\n",
    "It means you don't need to re-learn how to build a model every time you want to try out something new!\n",
    "Using the [Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model, you simply\n",
    "add each layer in the order you want, and compile it. Tensorflow takes care of everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and compile\n",
    "We're going to start with just one layer. Take another look at the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) to see all the configuration options for the Conv2D layer. As a first step we will stick to defining only the necessary arguments to the layer,\n",
    "leaving the rest as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the Sequential model, and add one Conv2D layer to it\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        input_shape=images.shape[1:] # Shape of a single image\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Add an output layer to the model\n",
    "# Recall the shape of the input to a dense neural network.\n",
    "# What is the shape of the output from the Conv2D layer?\n",
    "\n",
    "\n",
    "# Once the model is defined, we need to compile it. \n",
    "# This is where we specify the loss function, optimizer, and metrics if we want.\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We are ready to train the model. Remember to normalize your inputs if you haven't already done that!\n",
    "You can specify the validation data using the keyword 'validation_data'. Take a look at the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit) for the fit() function to see how it\n",
    "expect the validation to be formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 32\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model, save output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Side by side plots of losses and accuracies\n",
    "history = training_output.history\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].plot(history['loss'], label='training')\n",
    "ax[0].plot(history['val_loss'], label='validation')\n",
    "ax[0].set_title(\"Model losses\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(history['accuracy'], label='training')\n",
    "ax[1].plot(history['val_accuracy'], label='validation')\n",
    "ax[1].set_title(\"Model accuracy\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand and improve the model\n",
    "Just like with a feed forward neural network, the architecture is just one of the properties\n",
    "we can tune to improve performance. If you look closer at the model.summary() printout, you can\n",
    "see that the output after the Conv2D layer has changed shape. It's no longer the same size as our\n",
    "original image. Why is this?\n",
    "\n",
    "In the introduction we mentioned that the filters/kernels in a CNN\n",
    "extract features in the images. One thing we could do, then, is to input those features to an FFNN. Just\n",
    "like in the dense_neural_network notebook.\n",
    "\n",
    "In a way we've already done this, except the dense part of our model has only one node, the output node.\n",
    "Try to add another dense layer before the output. Perhaps you have a dense model from the previous notebook\n",
    "you can weave in here?\n",
    "\n",
    "Of course, you can also add another Conv2D layer instead, or increase the number of filters.\n",
    "In between convolutional layers you can add MaxPool2D layers, and eventually you may start looking at\n",
    "regularizations like Dropout if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize a filter\n",
    "We can extract a filter's 'weights' and visualize what one (or all, if you wish) filter extracts\n",
    "from an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import convolve2d\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "\n",
    "# Get filters from the first Conv2D layer\n",
    "filters, biases = model.layers[0].get_weights()\n",
    "print(filters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "So we have 32 3x3 filters, as expected since this is what we set the Conv2D layer to use.\n",
    "Notice, however, that the filters array doesn't have the the number of filters on the first axis. To extract\n",
    "a single filter from this array, we need to index like `tmp = filters[:,:,:,0]` to get the first filter.\n",
    "The next step is to pick a filter from the 32 filters we have available, and aplly it to a sample image.\n",
    "We can use SciPy's [convolve2](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html) to do this.\n",
    "\n",
    "NB! `filter` is a keyword in python, so if you extract a single filter, name the variable something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a filter and an image\n",
    "filter_idx = 0\n",
    "img_idx = 0\n",
    "\n",
    "# We need to reshape the filters and images for \n",
    "filtr = filters[:,:,:, filter_idx]\n",
    "image = images[img_idx]\n",
    "conv_image = convolve2d(\n",
    "    image.reshape(16, 16),\n",
    "    filtr.reshape(3, 3)\n",
    ")\n",
    "\n",
    "# Plot the images and filter\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax[0].imshow(image.reshape(16, 16), cmap='gray')\n",
    "ax[0].set_title(\"Original image\")\n",
    "\n",
    "ax[1].imshow(conv_image, cmap='gray')\n",
    "ax[1].set_title(\"Convolved image\")\n",
    "\n",
    "sns.heatmap(filtr.reshape(3, 3), cmap='gray', square=True, annot=True, ax=ax[2])\n",
    "ax[2].set_title(\"Filter\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

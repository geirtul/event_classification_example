{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import and exploration\n",
    "New pulsemaker files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, look at the data\n",
    "id, samples, amplitude1, rise1, decay1, position1, offset, amplitude2, rise2, decay2, position2, val1, val2, ...\n",
    "The id is either 1 for a single pulse or 2 for a double pulse. In the single pulse case all the pulse parameters for the second pulse are 0.\n",
    "\n",
    "Open up the file and look at it. In most linux terminals you can also use the command\n",
    "`head` to display a set number of lines from the start of the file. We'll start with the first line.\n",
    "Usually you will also have some information about the file formatting.\n",
    "You can run shell commands inside the notebook by starting the line with an exclamation mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head -1 ../data/training_pm_nosat_150k.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import\n",
    "This is a bit of a \"hands-on\" approach that focuses on using base functions in python and\n",
    "Numpy to read the file and create some arrays we can work with.\n",
    "Reading the file is pretty straightforward, but some challenges appear when the file is large.\n",
    "We will prepare for that scenario from the get-go.\n",
    "\n",
    "training_pm_nosat.dat contains 1M mixed single and double events.\\\n",
    "We set a datapath and filename. Note that we use the 'relative' path to our data, from the\n",
    "location of the notebook. This works well in a github repo where you control the folder structure.\n",
    "And if someone else will clone or fork the repo, they don't need to replace the paths.\n",
    "\n",
    "For small datafiles you can read the entire file into memory, store it as a list where each element is a line in the file, and work from there.\n",
    "```python\n",
    "with open(DATA_PATH + fname, \"r\") as datafile:\n",
    "    data = datafile.readlines()\n",
    "```\n",
    "You now have a list `data` containing all the lines in the file. Using the `with` statement is a shortcut so\n",
    "we don't need to manually close the file after opening it.\n",
    "However, reading the entire file into memory might not be possible, so we will read it line by line instead,\n",
    "looping over the file. This read only one line into memory at a time.\n",
    "\n",
    "Some resources for this exercise\n",
    "* [Python I/O File handling](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files)\n",
    "* [Numpy array indexing](https://numpy.org/doc/1.18/reference/arrays.indexing.html)\n",
    "* [Numpy's fromstring() function](https://numpy.org/doc/1.18/reference/generated/numpy.fromstring.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to data and filename. You can also store it a single variable\n",
    "# DATA_PATH = \"../data/CeBr10k_1.txt\" if you prefer. Here we expect to use at least the path itself later,\n",
    "# so we separate them.\n",
    "DATA_PATH = \"../data/\"\n",
    "fname = \"training_pm_nosat_150k.dat\"\n",
    "\n",
    "# Number of lines to import\n",
    "num_lines = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store each image, energy, position, and labels. We know the filesize, so we could create\n",
    "# arrays that perfectly match the data, but let's assume we don't know how many lines we're going to\n",
    "# read. \n",
    "labels = []\n",
    "amplitudes = []\n",
    "rises = []\n",
    "decays = []\n",
    "positions = []\n",
    "offsets = []\n",
    "samples = []\n",
    "\n",
    "# Open the file\n",
    "with open(DATA_PATH + fname, \"r\") as datafile:\n",
    "    # Loop over the file, line by line\n",
    "    linecount = 0\n",
    "    for line in datafile:\n",
    "        if linecount >= num_lines:\n",
    "            break\n",
    "        # The line is still a string when read from the file. We use numpys fromstring()\n",
    "        # to convert the line to a numpy array, specifying that each element is separated\n",
    "        # by a space. This does not convert the line in the file, only the \"copy\" that we have\n",
    "        # read into memory. fromstring() also removes any trailing newline ('\\n') characters\n",
    "        # so we don't have to worry about that. The values will be interpreted as floats.\n",
    "        line = np.fromstring(line, sep=' ')\n",
    "        \n",
    "        # id, samples, amplitude1, rise1, decay1, position1, offset, amplitude2, rise2, decay2, position2, val1, val2,\n",
    "        # First we grab the known variables for the data, and set labels to 0 / 1 for single / double\n",
    "        label = 0 if line[0] == 1 else 1\n",
    "        amplitude = np.array((line[2], line[7]))\n",
    "        rise = np.array((line[3], line[8]))\n",
    "        decay = np.array((line[4], line[9]))\n",
    "        pos = np.array((line[5], line[10]))\n",
    "        offset = line[6]\n",
    "        \n",
    "        # Grab the sample values, located at index 11 to the end of the line\n",
    "        sample = np.array(line[11:])\n",
    "\n",
    "        # Finally, we take the separated arrays and add them to their respective \"storage\" lists.\n",
    "        labels.append(label)\n",
    "        amplitudes.append(amplitude)\n",
    "        rises.append(rise)\n",
    "        decays.append(decay)\n",
    "        positions.append(pos)\n",
    "        offsets.append(offset)\n",
    "        samples.append(sample)\n",
    "        \n",
    "        linecount += 1\n",
    "\n",
    "        \n",
    "# We've now looped over the entire file. The only thing that remains is to convert the lists\n",
    "# to numpy arrays.\n",
    "labels = np.array(labels)\n",
    "amplitudes = np.array(amplitudes)\n",
    "rises = np.array(rises)\n",
    "decays = np.array(decays)\n",
    "positions = np.array(positions)\n",
    "offsets = np.array(offsets)\n",
    "samples = np.array(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print the shape of the arrays we've made as a quick check using\n",
    "# the shape property of numpy arrays.\n",
    "print(\"Labels shape, dtype:\", labels.shape, labels.dtype)\n",
    "print(\"Amplitues shape:\", amplitudes.shape, amplitudes.dtype)\n",
    "print(\"Rises shape:\", rises.shape, rises.dtype)\n",
    "print(\"Decays shape:\", decays.shape, decays.dtype)\n",
    "print(\"Positions shape:\", positions.shape, positions.dtype)\n",
    "print(\"Offsets shape:\", offsets.shape, offsets.dtype)\n",
    "print(\"Samples shape:\", samples.shape, samples.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above tells us that we've got 10000 images, each of length 256, as well as two energy values for each image, and four positions. The images, energies, and positions arrays have two dimensions, while the labels array\n",
    "only has one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection of imported data\n",
    "Let's plot a single and a double event to see what we're working with.\n",
    "We can extract the indices of singles and double using the labels array, and a numpy\n",
    "function called np.where.\n",
    "\n",
    "Resources:\n",
    "* [Matplotlib tutorial](https://matplotlib.org/tutorials/introductory/pyplot.html)\n",
    "* [Numpy np.where() doc](https://numpy.org/doc/stable/reference/generated/numpy.where.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where returns a tuple where the first (and in this case only) element is our indices\n",
    "singles = np.where(labels == 0)[0]\n",
    "doubles = np.where(labels == 1)[0]\n",
    "print(\"Num imported single events: \", singles.shape)\n",
    "print(\"Num imported double events: \", doubles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now grab a single and a double event by using the index arrays we made\n",
    "# Ex. to get the sample values for the first single event: samples[singles[0]]\n",
    "\n",
    "# Plot a single and double event side by side\n",
    "# Add red dotted lines for where the positions are.\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].plot(samples[singles[0]])\n",
    "ax[0].axvline(positions[singles[0], 0], color='r', linestyle='--')\n",
    "ax[0].set_title(\"Single event \")\n",
    "ax[0].set_xlabel(\"Sample number\")\n",
    "ax[0].set_ylabel(\"Amplitude\")\n",
    "\n",
    "ax[1].plot(samples[doubles[0]])\n",
    "ax[1].axvline(positions[doubles[0], 0], color='r', linestyle='--')\n",
    "ax[1].axvline(positions[doubles[0], 1], color='r', linestyle='--')\n",
    "ax[1].set_title(\"Double event\")\n",
    "ax[1].set_xlabel(\"Sample number\")\n",
    "ax[1].set_ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "# Print the additional information we have about the events.\n",
    "print(\"============ Single event info\")\n",
    "print(\"Amplitude:\", amplitudes[singles[0]])\n",
    "print(\"Rise:\", rises[singles[0]])\n",
    "print(\"Decay:\", decays[singles[0]])\n",
    "print(\"Positions:\", positions[singles[0]])\n",
    "print(\"Offset:\", offsets[singles[0]])\n",
    "\n",
    "print(\"============ Double event info\")\n",
    "print(\"Amplitude:\", amplitudes[doubles[0]])\n",
    "print(\"Rise:\", rises[doubles[0]])\n",
    "print(\"Decay:\", decays[doubles[0]])\n",
    "print(\"Positions:\", positions[doubles[0]])\n",
    "print(\"Offset:\", offsets[doubles[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting with annotations\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].plot(samples[singles[0]])\n",
    "ax[0].plot(positions[singles[0], 0], amplitudes[singles[0], 0], 'rx')\n",
    "ax[0].set_title(\"Single event\")\n",
    "ax[0].set_xlabel(\"Sample number\")\n",
    "ax[0].set_ylabel(\"Amplitude\")\n",
    "\n",
    "ax[1].plot(samples[doubles[0]])\n",
    "ax[1].plot(positions[doubles[0], 0], amplitudes[doubles[0], 0], 'rx')\n",
    "ax[1].plot(positions[doubles[0], 1], amplitudes[doubles[0], 1], 'rx')\n",
    "ax[1].set_title(\"Double event\")\n",
    "ax[1].set_xlabel(\"Sample number\")\n",
    "ax[1].set_ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate and save the data\n",
    "First, we separate the data into a *training set* and a *test set*. The test set will be used to give us an \"out-of-sample\"\n",
    "accuracy. To do this properly, the model will never see the test set in any way, shape, or form before we've trained, evaluated, and optimized it as far as we'd like to go. In other words, we use the training set to make the model as good as possible, and only then do we predict on the test set and report out-of-sample metrics.\n",
    "\n",
    "You can do the splitting of data manually by slicing the arrays, but scikit-learn has it's own function for doing this,\n",
    "[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn.model_selection.train_test_split).\n",
    "Instead of creating copies of the dataset, filling up often precious memory, we will work with indices that we\n",
    "pass around. This also makes it easier to trace any results back to the original inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for all data\n",
    "x_idx = np.arange(images.shape[0])\n",
    "\n",
    "# Split the indices into training and test sets (take out 10% of the data as test)\n",
    "train_idx, test_idx, not_used1, not_used2 = train_test_split(x_idx, x_idx, test_size = 0.1)\n",
    "\n",
    "# Save the training and test data in the data folder\n",
    "# We also need to save the labels, energies, and positions. This allows us to\n",
    "# quickly load it if we need it.\n",
    "\n",
    "# Save the training data. np.save adds a \".npy\" file extension to the provided filename.\n",
    "# We save the files in the same folder as the original datafile.\n",
    "np.save(DATA_PATH + \"images_training\", images[train_idx])\n",
    "np.save(DATA_PATH + \"energies_training\", energies[train_idx])\n",
    "np.save(DATA_PATH + \"positions_training\", positions[train_idx])\n",
    "np.save(DATA_PATH + \"labels_training\", labels[train_idx])\n",
    "\n",
    "# Save the test data\n",
    "np.save(DATA_PATH + \"images_test\", images[test_idx])\n",
    "np.save(DATA_PATH + \"energies_test\", energies[test_idx])\n",
    "np.save(DATA_PATH + \"positions_test\", positions[test_idx])\n",
    "np.save(DATA_PATH + \"labels_test\", labels[test_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also opt to store the entire training and test sets as two files, by concatenating them.\n",
    "This is just personal preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hdf5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open separate files for saving training and test datasets\n",
    "data_training = h5py.File(DATA_PATH + \"data_training.hdf5\", \"w\")\n",
    "data_test = h5py.File(DATA_PATH + \"data_test.hdf5\", \"w\")\n",
    "\n",
    "# Training data\n",
    "data_training.create_dataset('images', data=images[train_idx])\n",
    "data_training.create_dataset('energies', data=energies[train_idx])\n",
    "data_training.create_dataset('positions', data=positions[train_idx])\n",
    "data_training.create_dataset('labels', data=labels[train_idx])\n",
    "data_training.close()\n",
    "\n",
    "# Test data\n",
    "data_test.create_dataset('images', data=images[test_idx])\n",
    "data_test.create_dataset('energies', data=energies[test_idx])\n",
    "data_test.create_dataset('positions', data=positions[test_idx])\n",
    "data_test.create_dataset('labels', data=labels[test_idx])\n",
    "data_test.close()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
